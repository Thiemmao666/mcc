alloy:
  configMap:
    # -- Create a new ConfigMap for the config file.
    create: true
    # -- Content to assign to the new ConfigMap.  This is passed into `tpl` allowing for templating from values.
    content: |
      logging {
        level  = "debug"
        format = "logfmt"
       // timezone = "Asia/Ho_Chi_Minh"
      }
      
      discovery.kubernetes "nodes" {
        role = "node"
      }
      prometheus.scrape "kubelet" {
        job_name = "kubelet"
        scrape_interval = "15s"
        targets = discovery.kubernetes.nodes.targets
        forward_to = [prometheus.remote_write.endpoint.receiver]
        scheme = "http"
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
        tls_config {
          insecure_skip_verify = true
        }
        clustering {
          enabled = true
        }
      }
      prometheus.scrape "kubelet_cadvisor" {
        job_name = "kubelet_cadvisor"
        scrape_interval = "15s"
        targets = discovery.kubernetes.nodes.targets
        forward_to = [prometheus.remote_write.endpoint.receiver]
        scheme = "http"
        metrics_path = "/metrics/cadvisor"
        bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
        tls_config {
          insecure_skip_verify = true
        }
        clustering {
          enabled = true
        }
      }
      prometheus.exporter.unix "node_exporter" {}
      discovery.relabel "node_exporter" {
        targets = prometheus.exporter.unix.node_exporter.targets
        rule {
          action = "labeldrop"
          regex = "job"
        }
      }

      prometheus.scrape "node_exporter" {
        job_name = "node_exporter"
        scrape_interval = "15s"
        targets = discovery.relabel.node_exporter.output
        forward_to = [prometheus.remote_write.endpoint.receiver]
      }  

      remote.kubernetes.secret "postgres_credentials" {
        namespace = "mcc"
        name = "pgpool"
      }
      
      prometheus.exporter.postgres "postgres_exporter" {
        data_source_names = ["postgresql://" + remote.kubernetes.secret.postgres_credentials.data["PGPOOL_POSTGRES_USERNAME"] + ":" + remote.kubernetes.secret.postgres_credentials.data["PGPOOL_POSTGRES_PASSWORD"] + "@10.244.2.129:5432/postgres?sslmode=disable"]
        autodiscovery {
          enabled = true
        }
        custom_queries_config_path = "/hostfs/opt/custom-queries.yaml"
      }
      // prometheus.exporter.postgres "postgres_exporter_second" {
      //   data_source_names = ["postgresql://" + remote.kubernetes.secret.postgres_credentials.data["PGPOOL_POSTGRES_USERNAME"] + ":" + remote.kubernetes.secret.postgres_credentials.data["PGPOOL_POSTGRES_PASSWORD"] + "@10.244.2.50:5432/postgres?sslmode=disable"]
      //   autodiscovery {
      //     enabled = true
      //   }
      //  }
      discovery.relabel "postgres_exporter" {
        targets = prometheus.exporter.postgres.postgres_exporter.targets
        rule {
          action = "labeldrop"
          regex = "job"
        }
      }

      //  discovery.relabel "postgres_exporter_second" {
      //    targets = prometheus.exporter.postgres.postgres_exporter_second.targets
      //   rule {
      //     action = "labeldrop"
      //     regex = "job"
      //   }
      //  }

      prometheus.scrape "postgres_exporter" {
        job_name = "postgres_exporter"
        scrape_interval = "15s"
        targets    = discovery.relabel.postgres_exporter.output
        forward_to = [prometheus.remote_write.endpoint.receiver]
        clustering {
          enabled = true
        }
      }

      // prometheus.scrape "postgres_exporter_second" {
      //   job_name = "postgres_exporter_second"
      //   scrape_interval = "15s"
      //   targets    = discovery.relabel.postgres_exporter_second.output
      //   forward_to = [prometheus.remote_write.endpoint.receiver]
      //   clustering {
      //     enabled = true
      //   }
      //  }

      prometheus.scrape "kube_state_metrics" {
        job_name = "kube_state_metrics"
        scrape_interval = "15s"
        targets = [{"__address__" = "kube-state-metrics.test.svc.cluster.local:8080"},]
        forward_to = [prometheus.remote_write.endpoint.receiver]
        clustering {
          enabled = true
        }
      }



      livedebugging {
        enabled = true
      }
      
      
      // Discovers all kubernetes pods.
      // Relies on serviceAccountName=grafana-alloy in the pod spec for permissions.
      discovery.kubernetes "pods" {
        selectors {
          field = "spec.nodeName=" + env("HOSTNAME")
          role = "pod"
        }
        role = "pod"
      }
      
      // Discovers all processes running on the node.
      // Relies on a security context with elevated permissions for the alloy container (running as root).
      // Relies on hostPID=true on the pod spec, to be able to see processes from other pods.
      discovery.process "all" {
        // Merges kubernetes and process data (using container_id), to attach kubernetes labels to discovered processes.
        //join = discovery.kubernetes.pods.targets
        // root = "/hostfs"
        join = discovery.kubernetes.pods.targets
      }

      // Drops non-java processes and adjusts labels.    
      discovery.relabel "java" {
        targets = discovery.process.all.targets

        // Drops non-java processes.
        rule {
          source_labels = ["__meta_process_exe"]
          action = "keep"
          regex = ".*/java$"
        }
     
        // Drop any targets that do not have the value "neo" in their "__meta_kubernetes_namespace" label.
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          action        = "keep"
          regex         = "neo"
        }

        // Sets up the service_name using the namespace and container names.
        rule {
          source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
          target_label = "service_name"
          separator = "/"
        }

        // Sets up kubernetes labels (labels with the __ prefix are ultimately dropped).
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_node_name"]
          target_label = "node"
        }

        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_namespace"]
          target_label = "namespace"
        }

        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_name"]
          target_label = "pod"
        }

        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_container_name"]
          target_label = "container"
        }

        // Sets up the cluster label.
        // Relies on a pod-level annotation with the "cluster_name" name.
        // Alternatively it can be set up using external_labels in pyroscope.write. 
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_annotation_cluster_name"]
          target_label = "cluster"
        }
      }

      // Attaches the Pyroscope profiler to the processes returned by the discovery.relabel component.
      // Relies on a security context with elevated permissions for the alloy container (running as root).
      // Relies on hostPID=true on the pod spec, to be able to access processes from other pods.
      pyroscope.java "java" {
        profiling_config {
          interval = "15s"
          alloc = "512k"
          cpu = true
          lock = "10ms"
          sample_rate = 100
        }
        forward_to = [pyroscope.write.local.receiver]
        targets = discovery.relabel.java.output
      }
        
      pyroscope.write "local" {
        // Send metrics to the locally running Pyroscope instance.
        endpoint {
          url = "http://pyroscope:4040"
        }
        external_labels = {
          "static_label" = "static_label_value",
        }
      }

      loki.source.kubernetes "pods" {
        targets    = discovery.kubernetes.pods.targets
        forward_to = [loki.write.endpoint.receiver]
      }

      loki.write "endpoint" {
        endpoint {
            url = "http://loki:3100/loki/api/v1/push"
            tenant_id = "local"
        }
      }
      prometheus.remote_write "endpoint" {
        endpoint {
            url = "http://kube-prometheus-prometheus:9090/api/v1/write"
        }
      }
      

      prometheus.remote_write "endpoint_thanos" {
        endpoint {
            url = "http://kube-prometheus-prometheus-thanos:10902/api/v1/receive"
        }
      }

      otelcol.receiver.otlp "receiver" {
        grpc {
            endpoint = "0.0.0.0:4317"
        }
        http {endpoint = "0.0.0.0:4318"}
        output {
          traces  = [otelcol.processor.batch.batch.input,otelcol.connector.servicegraph.default.input,]
          metrics = [otelcol.processor.batch.batch.input]
          logs    = [otelcol.processor.batch.batch.input]
          }
        }


      otelcol.processor.batch "batch" {
        output {
          traces  = [otelcol.exporter.otlp.to_tempo.input]
          logs    = [otelcol.exporter.loki.oltp_loki.input]
          metrics = [otelcol.exporter.prometheus.oltp_prometheus.input]
        }
      }

      otelcol.exporter.loki "oltp_loki" {
        forward_to = [loki.write.endpoint.receiver]
      }
      otelcol.exporter.prometheus "oltp_prometheus" {
        forward_to = [prometheus.remote_write.endpoint.receiver]
      }

      otelcol.connector.servicegraph "default" {
        dimensions = ["http.method", "http.target"]
        output {
          metrics = [otelcol.exporter.otlp.to_tempo.input]
        }
      }

      otelcol.exporter.otlp "to_tempo" {
        client {
          endpoint = "http://tempo-gateway:4317"
          tls {
            insecure = true
          }
        }
      }




     # otelcol.pipeline.trace "tempo" {
      #  forward_to = [prometheus.remote_write.endpoint.receiver]
      #  receivers = [otelcol.receiver.otlp.receiver]
      #  processors = [batch.processor]
      #}
      #batch "processor" {
      #  timeout = "5s"
      #  send_batch_size = 512
      #}
     
#  //        metrics = [otelcol.exporter.prometheus.to_prom.input]
# //          logs    = [otelcol.exporter.loki.to_loki.input] 
    # -- Name of existing ConfigMap to use. Used when create is false.
